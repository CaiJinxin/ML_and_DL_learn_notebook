{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机\n",
    "支持向量机是强有力的使用广泛的机器学习模型，可以支持线性和非线性的分类，回归，甚至离群值检测。  \n",
    "支持向量机特别适合复杂的小型或中性数据集。  \n",
    "## 线性SVM分类\n",
    "支持向量机的核心思想是使用一个超平面将类别分割开，并且使最近的两个样例点与平面距离达到最大。  \n",
    "SVM分类将类别用一条尽可能宽的街道分隔开，这叫做大间隔分类方法。  \n",
    "不在分隔道的样例不会对svm产生任何影响，它只取决于在分隔道边的样例，这些样例被称为支持向量  \n",
    "支持向量机方法还对样例数值范围敏感，所以使用前需要scale。  \n",
    "## 软间隔分类  \n",
    "严格要求所有样例都在分隔道外的方法称为硬间隔hard margin classification，这种方法对于离群值很敏感。  \n",
    "为了避免因为离群值导致无法间隔分类，我们限制分隔违反margin violations，这种方法就是soft margin classification。  \n",
    "在sklearn中使用超参数C来描述分隔违反，C越小，分隔道越宽，分隔违反越严重。  \n",
    "如果SVM过拟合，可以正则化C来抑制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T02:47:20.411337Z",
     "start_time": "2018-03-14T02:47:19.965105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)\n",
    "\n",
    "svm_clf = Pipeline((\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss='hinge')),\n",
    "        ))\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与其他分类方法不同，svm不能输出样例对于每个类的概率。  \n",
    "也可以使用SVC(kenerl=\"linear\",C=1)来实现，不过这种方法很慢，特别是面对大量数据的情况，  \n",
    "另一个方法就是用SGDClassifier(loss=\"hinge\",alpha=1/(m*C)，这种方法应用SGD去训练线性SVM模型，他不一定比linearsvc快，但是在处理海量数据，或者应用在线学习时非常有用。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非线性SVM分类\n",
    "## polyfeature方法\n",
    "有些数据集不是线性可分的，而使用多次项作为新特征又可分，类似多项式线性分类：  \n",
    "sklearn中我们可以在pipeline中加入polyfeature来实现这种分类。\n",
    "## polynoiaml kernal方法\n",
    "加入多项式特征可以很好的应用在机器学习的各种算法上，但是低degree不能处理复杂数据，过高degree又导致新建特征太多，很容易模型过慢。 \n",
    "幸运的svm有核方法kernal trick。我们可以做出和多项式特征方法一样的结果。coef0参数控制高次项对低次项的影响比例。\n",
    "## 加入相似特征\n",
    "另一个处理非线性问题的办法是使用相似的函数加入新特征，从而测量每个样例相似的位置。\n",
    "下限情况是m个样例加入m的特征来区分  \n",
    "高斯rbf核  \n",
    "$\\phi\\gamma(x,\\ell)=exp(-\\gamma||x-\\ell||^2)$  \n",
    "## 如何选择合适的核\n",
    "首先尝试linear线性核，特别是训练集比较大或者有足够的特征时；  \n",
    "当训练集比较小时，可以尝试RBF核，通常很有用，  \n",
    "如果你还有足够的时间，再去尝试其他核，使用cv和gridsearch选择合适的参数。\n",
    "# SVM回归\n",
    "和SVC不同，SVR是尽可能的使样例出现在分隔道上，道宽为超参数$\\epsilon$,预测曲线就是分隔道中心线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM的机制\n",
    "## 决策函数和预测\n",
    "SVM中我们将各项特征的系数向量称为$\\mathbf w$，截距称作$b$,则有方程如下:      \n",
    "$\\hat{y}=\\left{\\begin{array}{ll}0&if\\quad\\mathbf w^T\\cdot x+b<0,\\\\1&if\\quad\\mathbf w^T\\cdot x+b\\ge0\\end{array}\\right.$    \n",
    "  \n",
    "  其中$\\mathbf w^T\\cdot x+b是决策函数$，实际上分隔道中心线就是决策函数与特征空间重叠的部分。  \n",
    "## 训练目标\n",
    "越大的斜率$||\\mathbf w||$导致越小的间隔。如果要避免间隔违反即实现hard margin，我们要使决策函数在所有正例输入下，输出大于1，反例输入下，输出小于-1。如果我们定义  \n",
    "$t^{(i)}=\\left\\{\\begin{array}{ll} 1&if\\quad y^{(i)}=1\\\\-1&if\\quad y^{(i)}=0\\end{array}\\right.$  \n",
    "那么有硬分隔目标如下：  \n",
    "$minimize_{\\mathbf w,b}\\qquad \\frac{1}{2}\\mathbf w^T\\cdot\\mathbf w$  \n",
    "$subject\\quad to\\quad t^{(i)}(\\mathbf w^T\\cdot x^{(i)}+b)\\ge 1 \\quad for\\quad i=1,2\\cdots ,m$  \n",
    "  \n",
    "为了实现软分隔，我们加入从变量$\\zeta^{(i)}\\ge 0$用来表示第i个样例允许的间隔违反程度。我们现在有两个相冲突的目标，一个是尽量减少违反，一个是加大间隔。引进C系数作为其中的折中参数。目标函数如下：  \n",
    "$minimize_{\\mathbf w,b,\\zeta}\\qquad \\frac{1}{2}\\mathbf w^T\\cdot\\mathbf w+C\\sum_{i=1}^m\\zeta^{(i)}$  \n",
    "$subject\\quad to\\quad t^{(i)}(\\mathbf w^T\\cdot x^{(i)}+b)\\ge 1-\\zeta^{(i)}\\qquad and\\qquad \\zeta^{(i)}\\ge0 \\quad for\\quad i=1,2\\cdots ,m$    \n",
    "## 二次规划\n",
    "上述两个目标函数都是线性约束条件下的二次曲线优化问题，被称作二次规划，一般表达式  \n",
    "$ minimize_p \\quad \\frac{1}{2}\\mathbf p^T\\cdot \\mathbf H\\cdot\\mathbf p +\\mathbf f^T\\cdot p $  \n",
    "$ subject\\quad to\\quad\\mathbf A\\cdot p\\le b $  \n",
    "$ where\\left{\\begin{array}{ll}\\mathbf p& \\text{is an $n_p$-dimensional vector($n_p$=number of parameters)},\\\\\n",
    "\\mathbf H&\\text{is an $n_p \\times n_p$ matrix},\\\\\n",
    "\\mathbf f&\\text{is an $n_p$-dimensional vector},\\\\\n",
    "\\mathbf A&\\text{is an $n_c\\times n_p$ matrix($n_c$=number of constraints)},\\\\\n",
    "\\mathbf b&\\text{is an $n_c$-dimensional vector}. \\end{array}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对偶问题\n",
    "给定一个规划问题可以用表达不同但是紧密相关的另一个问题表示，称为对偶问题。通常情况下对偶问题只能求出原问题的下限，在某些情况下，对偶问题的解就是原问题的解，而SVM就包含了这些条件(目标函数是凸函数，约束条件是连续凸函数)  \n",
    "SVM对偶的目标函数:  \n",
    "$minimize_\\alpha\\quad \\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha^{(i)}\\alpha^{(j)}t^{(i)}t^{(j)}x^{(i)T}\\cdot x^{(j)} - \\sum_{i=1}^m\\alpha^{(i)} $  \n",
    "$\\text{subject to }\\alpha^{(i)}\\ge0\\text{ for }i=1,2,\\cdots,m  $  \n",
    "一旦找到了合适的$\\hat{\\alpha}$,就能利用下列等式计算$\\hat{\\mathbf w},\\hat{b}$  \n",
    "$\\hat{\\mathbf w}=\\sum_{i=1}^m\\hat{\\alpha}^{(i)}t^{(i)}x^{(i)}$  \n",
    "$\\hat{b}=\\frac{1}{n_s}\\sum_{i=1,\\hat{\\alpha}^{(i)}>0}^m(1-t^{(i)}(\\hat{\\mathbf w}^T\\cdot x^{(i)}))$  \n",
    "在样例数量比特征数量小时，求对偶问题比求解原问题更加快速。更重要的是，对偶问题可以使用核技巧。 \n",
    "## 核技巧\n",
    "二次多项式映射：  \n",
    "$\\phi(\\mathbf x)=\\phi\\left(\\left(\\begin{array}{l} x_1\\\\x_2 \\end{array}\\right)\\right)=\\left(\\begin{array}{l}x_1^2\\\\\\sqrt2x_1x_2\\\\x_2^2\\end{array}\\right)$   \n",
    "$\\phi(\\mathbf a)^T\\cdot\\phi(\\mathbf b)=\\left(\\begin{array}{l}a_1^2\\\\\\sqrt2a_1a_2\\\\a_2^2\\end{array}\\right)\\left(\\begin{array}{l}b_1^2\\\\\\sqrt2b_1b_2\\\\b_2^2\\end{array}\\right)=a_1^2b_1^2+2a_1b_1a_2b_2+a_2^2b_2^2=(a_1b_1+a_2b_2)^2=(\\mathbf a^T\\cdot \\mathbf b)^2$  \n",
    "变换结果的点乘变成了原向量的点乘后平方！  \n",
    "核技巧就是对所有样例应用变换，求对等问题时变换后的点乘用原样例点乘的平方替换，这样就可以不用实际对所有样例应用变换了，而且还将问题变成线性分隔问题，所以这使得计算变得效率。这种变换函数称为核函数，类似二次变换的$K(\\mathbf a,\\mathbf b)=(\\mathbf a^T\\cdot \\mathbf b)^2$很多，下面我们列举一些常用的：  \n",
    "$\\text{Linear: }K(\\mathbf a,\\mathbf b)=\\mathbf a^T\\cdot \\mathbf b $  \n",
    "$\\text{Polynomial: }K(\\mathbf a,\\mathbf b)=(\\gamma \\mathbf a^T\\cdot\\mathbf b+r)^d $  \n",
    "$\\text{Gaussian RBF: }K(\\mathbf a,\\mathbf b)=exp(-\\gamma||\\mathbf a-\\mathbf b||^2)  $  \n",
    "$\\text{Sigmoid: }K(\\mathbf a,\\mathbf b)=tanh(\\gamma\\mathbf a^T\\cdot \\mathbf b+r) $  \n",
    "应用核技巧求解对偶公式的时候，结果包含了$\\phi(x^{(i)})$。实际上，$\\hat{\\mathbf w}$和$\\phi(x^{(i)})$有相同的维数，这可能非常大，甚至无穷大，所以是无法计算的，那我们在不知道$\\hat{\\mathbf w}$怎么预测呢？  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个好办法就是将它塞入新样例向量$\\mathbf x^{(n)}$的判决函数，然后将得到输入向量点乘的一个表达式:  \n",
    "$h_{\\hat{\\mathbf w},\\hat b}(\\phi(\\mathbf x^{(n)})+\\hat b=(\\sum_{i=1}^m\\hat\\alpha^{(i)}t^{(i)}\\phi(\\mathbf x^{(i)}))^T\\cdot\\phi(\\mathbf x^{(n)})+\\hat b\\\\ \\qquad=\\sum_{i=1}^m\\hat\\alpha^{(i)}t^{(i)}(\\phi(\\mathbf x^{(i)})^T\\cdot\\phi(\\mathbf x^{(n)}))+\\hat b \\\\ \\qquad=\\sum_{i=1,\\hat\\alpha^{(i)}>0}^m\\hat\\alpha^{(i)}t^{(i)}K(\\mathbf x^{(i)},\\mathbf x^{(n)})+\\hat b$   \n",
    "注意只有支持向量的$\\hat\\alpha^{(i)}不为0$。因此每次加入新样例计算时，只调用支持向量的数据，而不是整个数据集，偏差项$\\hat b$的计算也是如此：  \n",
    "$\\hat b=\\frac{1}{n_s}\\sum_{i=1,\\hat\\alpha^{(i)}>0}^m(1-t^{(i)}\\hat{\\mathbf w}^T\\cdot\\phi(\\mathbf x^{(i)}))\n",
    "\\\\\\quad=\\frac{1}{n_s}\\sum_{i=1,\\hat\\alpha^{(i)}>0}^m(1-t^{(i)}(\\sum_{j=1}^m\\hat\\alpha^{(j)}t^{(j)}\\phi(\\mathbf x^{(j)}))^T\\cdot\\phi(\\mathbf x^{(i)}))\\\\\n",
    "\\quad=\\frac{1}{n_s}\\sum_{i=1,\\hat\\alpha^{(i)}>0}^m(1-t^{(i)}\\sum_{j=1,\\hat\\alpha{(j)}>0}^m\\hat\\alpha^{(j)}t^{(j)}K(\\mathbf x^{(i)},\\mathbf x^{(j)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在线SVMs\n",
    "对于线性SVM，我们可以应用BGD方法最小化损失函数，但是这比QP方法慢很多。  \n",
    "线性SVM分类的损失函数  \n",
    "$J(\\mathbf w,b)=\\frac{1}{2}\\mathbf w^T\\cdot\\mathbf w+C\\sum_{i=1}^mmax(0,1-t^{(i)}(\\mathbf w^T\\cdot \\mathbf x^{(i)}+b))$  \n",
    "第一个和项使用小权值的$\\mathbf w$向量，主导扩大分隔距离。第二个和项计算间隔违反。  \n",
    "$max(0,1-t)$称为铰链损失函数hinge loss  \n",
    "  \n",
    "还有一些kernel SVM可以用于在线算法，但是在处理大量分线性数据时，应该考虑神经网络来替代它。"
   ]
  }
 ],
 "metadata": {
  "author": "",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
