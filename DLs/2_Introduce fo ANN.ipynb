{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要\n",
    "ANNs(Artificial neural networks)人工神经网络是深度学习的核心技术，适用于高度复杂的机器学习任务，比如分类百万级的图片(Google Image),语音识别(Apple's Siri),或者是打败世界冠军(Alpha Go).\n",
    "# 感知机\n",
    "只有一层输入层(包含Bias Neuron和input Neuraon)和一层输出层(LTU).其中LTU：  \n",
    "$h_{\\mathbf w}(x)=step(z)=step(\\mathbf w^T\\cdot\\mathbf x)$  \n",
    "Heaviside step function  或者是符号函数  \n",
    "$haviside(z)=\\left{\n",
    "\\begin{array}{ll}\n",
    "0&\\text {if }z<0\\\\\n",
    "1&\\text {if }z\\ge0\\\\ \n",
    "\\end{array}\n",
    "\\right.$  \n",
    "\n",
    "$sgn(z)=\\left{\n",
    "\\begin{array}{ll}\n",
    "1&\\text {if }z<0\\\\\n",
    "0&\\text {if }z=0\\\\ \n",
    "-1&\\text {if }z<0\n",
    "\\end{array}\n",
    "\\right. $  \n",
    "感知机的训练规则：误差逆传播，每次只输入一个样例，分别对他们做出预测，如果出现错误预测，就加强导致输出错误的输入神经元连接权值。    \n",
    "\n",
    "$w_{i,j}^{(next step)}=w_{i.j}+\\eta(\\hat y_j-y_j)x_i$  \n",
    "$w_{i,j}是第i个输入神经元和第j个输出神经元间的连接权值$  \n",
    "$x_i是当前样例的第i个输入神经元的值$  \n",
    "$\\hat y_j是当前样例第j个输出神经元的输出值$  \n",
    "$y_j是当前样例的第j个输出神经元的目标值$  \n",
    "$\\eta 是学习率$  \n",
    "感知机的每个输出神经元的决策限是线性的，所以不能学习复杂的模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T03:48:47.064339Z",
     "start_time": "2018-03-19T03:48:46.488604Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\machine-learn\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sklearn提供Perception类,是一个单一的LTU网络  \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2, 3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机实际就是SGD的集成模式，实际上它等价于SGD,其中loss='perception'，learning_rate='constant'.eta0=1,penalty=None的情况.  \n",
    "注意到与逻辑回归分类相反的是，感知机不输出每个类的概率，它只基于阈值给出预测结果。所以逻辑回归比感知机优秀。  \n",
    "单层的神经网络分类效果很差，甚至连XOR问题都解决不了。  \n",
    "# 多层感知和逆向传播\n",
    "多层感知机包含有一个或多个由LTU构成的中间层(隐层)，除了输出层外，每层都有偏置神经元，并且每层之间全连接。当ANN含有两个或更多隐层时，称作DNN（deep neural networl深度神经网络).  \n",
    "多年以来，研究者都无法找到训练MLPs的方法。知道逆向传播算法的出现。如今我们使用reverse-mode autodiff（逆向自动微分）的梯度下降来描述这种算法。  \n",
    "  \n",
    "对于每一个训练样例，这个算法反馈到网络并计算每个层的输出。然后测算每个层的输出误差。然后计算上一层的每个神经元对该层输出误差的贡献，逐个往前直到输入层。使用这种逆向的方式对每个连接权值求误差对其的梯度。这种方式对逆向自动微分来说十分简单。最后的调整权值来减小误差。\n",
    "  \n",
    "为了让算法可以工作，需要在MLPs结构上做关键改动：将step函数改为逻辑函数$\\sigma(z)=1/(1+e^{-z})$,这就是的梯度可求，逆向传播算法还有其他的激活函数，可用来替代，比如:  \n",
    "hyperbolic tangent function  $tanh(z)=2\\sigma(2z)-1$  \n",
    "一样是s型可微函数，但是输出是-1到1,使得每个蹭的输出对于训练开始时越来越规范化，这有助于加速收敛。  \n",
    "ReLU function $ReLU(z)=max(0,z)$  \n",
    "连续不可微。然而在实际应用中表现很好，可以加快运算，最重要的是，输出没有上线，可以减少一些梯度下降的问题。  \n",
    "  \n",
    "一个MLP通常用来分类，因为每个输出是二分类的,当面对多分类的问题时，可以在最后层输出加入softmax函数，每个输出神经元对应输出类的概率。单向船体的神经网络结构称为前馈神经网络(feedforward neural netweorFNN)  \n",
    "# 使用TF的高级API训练MLP\n",
    "一个训练MLP的最简单方法就是使用TF的高级API-TF.Learn。和sklearn很相似。DNNClassifier创建一个DNN神经网络包含若干隐层，输出可以对类别做概率求值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T09:00:54.331267Z",
     "start_time": "2018-03-19T09:00:54.326266Z"
    }
   },
   "outputs": [],
   "source": [
    "#为了使用tb\n",
    "#from datetime import datetime\n",
    "\n",
    "#now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = 'tf_Logs'\n",
    "logdir = \"{}/2_Introduce for ANN/\".format(root_logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T07:33:50.736532Z",
     "start_time": "2018-03-19T07:31:50.553848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\CAIJIN~1\\AppData\\Local\\Temp\\tmpqsdocf51\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DA8896DFD0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\CAIJIN~1\\\\AppData\\\\Local\\\\Temp\\\\tmpqsdocf51'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\CAIJIN~1\\AppData\\Local\\Temp\\tmpqsdocf51\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.2543008, step = 1\n",
      "INFO:tensorflow:global_step/sec: 300.992\n",
      "INFO:tensorflow:loss = 0.051839646, step = 101 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 313.258\n",
      "INFO:tensorflow:loss = 0.013868751, step = 201 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.621\n",
      "INFO:tensorflow:loss = 0.0042499676, step = 301 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 359.458\n",
      "INFO:tensorflow:loss = 0.0035973936, step = 401 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.521\n",
      "INFO:tensorflow:loss = 0.003648219, step = 501 (0.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.062\n",
      "INFO:tensorflow:loss = 0.0020222964, step = 601 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.108\n",
      "INFO:tensorflow:loss = 0.0022528085, step = 701 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.359\n",
      "INFO:tensorflow:loss = 0.0022521326, step = 801 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 358.169\n",
      "INFO:tensorflow:loss = 0.0012638925, step = 901 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.758\n",
      "INFO:tensorflow:loss = 0.001556784, step = 1001 (0.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.359\n",
      "INFO:tensorflow:loss = 0.0011165452, step = 1101 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.869\n",
      "INFO:tensorflow:loss = 0.0011065858, step = 1201 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 377.092\n",
      "INFO:tensorflow:loss = 0.0007677876, step = 1301 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.04\n",
      "INFO:tensorflow:loss = 0.0007726662, step = 1401 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 353.105\n",
      "INFO:tensorflow:loss = 0.0007449863, step = 1501 (0.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 372.874\n",
      "INFO:tensorflow:loss = 0.0006769825, step = 1601 (0.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 364.706\n",
      "INFO:tensorflow:loss = 0.0008146359, step = 1701 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 384.344\n",
      "INFO:tensorflow:loss = 0.0003962952, step = 1801 (0.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.62\n",
      "INFO:tensorflow:loss = 0.00085018895, step = 1901 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.517\n",
      "INFO:tensorflow:loss = 0.000592273, step = 2001 (0.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.521\n",
      "INFO:tensorflow:loss = 0.0005114857, step = 2101 (0.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.4\n",
      "INFO:tensorflow:loss = 0.00045650563, step = 2201 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 390.35\n",
      "INFO:tensorflow:loss = 0.00069241656, step = 2301 (0.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 353.108\n",
      "INFO:tensorflow:loss = 0.0005889726, step = 2401 (0.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 353.105\n",
      "INFO:tensorflow:loss = 0.00034847236, step = 2501 (0.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.043\n",
      "INFO:tensorflow:loss = 0.00040605903, step = 2601 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.108\n",
      "INFO:tensorflow:loss = 0.000223696, step = 2701 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.777\n",
      "INFO:tensorflow:loss = 0.00040808844, step = 2801 (0.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 360.756\n",
      "INFO:tensorflow:loss = 0.000420603, step = 2901 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.913\n",
      "INFO:tensorflow:loss = 0.00020814448, step = 3001 (0.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.043\n",
      "INFO:tensorflow:loss = 0.00042119104, step = 3101 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.362\n",
      "INFO:tensorflow:loss = 0.00022993636, step = 3201 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.108\n",
      "INFO:tensorflow:loss = 0.0002570716, step = 3301 (0.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 349.403\n",
      "INFO:tensorflow:loss = 0.00027121272, step = 3401 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.618\n",
      "INFO:tensorflow:loss = 0.00030207454, step = 3501 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 362.063\n",
      "INFO:tensorflow:loss = 0.00035111626, step = 3601 (0.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.362\n",
      "INFO:tensorflow:loss = 0.00028898154, step = 3701 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.775\n",
      "INFO:tensorflow:loss = 0.00022953839, step = 3801 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 336.464\n",
      "INFO:tensorflow:loss = 0.00026715558, step = 3901 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.399\n",
      "INFO:tensorflow:loss = 0.00019117608, step = 4001 (0.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 342.224\n",
      "INFO:tensorflow:loss = 0.00021354004, step = 4101 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.107\n",
      "INFO:tensorflow:loss = 0.00027932605, step = 4201 (0.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 358.172\n",
      "INFO:tensorflow:loss = 0.00024058565, step = 4301 (0.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.89\n",
      "INFO:tensorflow:loss = 0.00022840816, step = 4401 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 370.108\n",
      "INFO:tensorflow:loss = 0.0002516473, step = 4501 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.776\n",
      "INFO:tensorflow:loss = 0.00021441575, step = 4601 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 375.674\n",
      "INFO:tensorflow:loss = 0.00024369295, step = 4701 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 387.321\n",
      "INFO:tensorflow:loss = 0.00020409572, step = 4801 (0.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.892\n",
      "INFO:tensorflow:loss = 8.3273684e-05, step = 4901 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 364.706\n",
      "INFO:tensorflow:loss = 0.00020581002, step = 5001 (0.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 368.741\n",
      "INFO:tensorflow:loss = 0.00026346647, step = 5101 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 333.099\n",
      "INFO:tensorflow:loss = 0.00020537802, step = 5201 (0.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 366.039\n",
      "INFO:tensorflow:loss = 0.00016851704, step = 5301 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 348.188\n",
      "INFO:tensorflow:loss = 0.00014658467, step = 5401 (0.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 321.316\n",
      "INFO:tensorflow:loss = 0.00023651606, step = 5501 (0.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 312.279\n",
      "INFO:tensorflow:loss = 0.00017324103, step = 5601 (0.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 306.532\n",
      "INFO:tensorflow:loss = 0.00020657084, step = 5701 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.914\n",
      "INFO:tensorflow:loss = 0.00020794634, step = 5801 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.064\n",
      "INFO:tensorflow:loss = 0.0002470391, step = 5901 (0.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 354.361\n",
      "INFO:tensorflow:loss = 0.00017248277, step = 6001 (0.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.4\n",
      "INFO:tensorflow:loss = 0.00014594305, step = 6101 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.957\n",
      "INFO:tensorflow:loss = 9.486571e-05, step = 6201 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.775\n",
      "INFO:tensorflow:loss = 0.00012342964, step = 6301 (0.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 356.892\n",
      "INFO:tensorflow:loss = 9.5388685e-05, step = 6401 (0.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 343.399\n",
      "INFO:tensorflow:loss = 0.0001442554, step = 6501 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 328.715\n",
      "INFO:tensorflow:loss = 0.00017924234, step = 6601 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 364.705\n",
      "INFO:tensorflow:loss = 9.548703e-05, step = 6701 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 367.388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 7.460798e-05, step = 6801 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 374.265\n",
      "INFO:tensorflow:loss = 0.00019611201, step = 6901 (0.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 363.381\n",
      "INFO:tensorflow:loss = 0.000102598606, step = 7001 (0.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.912\n",
      "INFO:tensorflow:loss = 0.000111172885, step = 7101 (0.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 384.344\n",
      "INFO:tensorflow:loss = 0.00011363651, step = 7201 (0.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 378.52\n",
      "INFO:tensorflow:loss = 0.00011220418, step = 7301 (0.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 350.629\n",
      "INFO:tensorflow:loss = 0.00016721395, step = 7401 (0.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 344.583\n",
      "INFO:tensorflow:loss = 0.00011923719, step = 7501 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.152\n",
      "INFO:tensorflow:loss = 0.00011837533, step = 7601 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 355.622\n",
      "INFO:tensorflow:loss = 0.00010915144, step = 7701 (0.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 351.863\n",
      "INFO:tensorflow:loss = 7.699353e-05, step = 7801 (0.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 325.503\n",
      "INFO:tensorflow:loss = 0.00011149267, step = 7901 (0.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 323.396\n",
      "INFO:tensorflow:loss = 8.447463e-05, step = 8001 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.125\n",
      "INFO:tensorflow:loss = 0.0001470148, step = 8101 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.125\n",
      "INFO:tensorflow:loss = 0.00011605035, step = 8201 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 398.125\n",
      "INFO:tensorflow:loss = 9.507716e-05, step = 8301 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 394.977\n",
      "INFO:tensorflow:loss = 8.382196e-05, step = 8401 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 393.421\n",
      "INFO:tensorflow:loss = 0.00015099549, step "
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10019 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load data\n",
    "X = load_digits().data.astype(np.int)\n",
    "y = load_digits().target.astype(np.int)\n",
    "\n",
    "#scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42, test_size=0.8, shuffle=True)\n",
    "\n",
    "#model construct\n",
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10,  #两个隐层一个300个神经元，另一个100个\n",
    "                                        feature_columns=feature_columns)\n",
    "#model train\n",
    "dnn_clf.fit(x=X_train, y=y_train, batch_size=50, steps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T07:33:51.266908Z",
     "start_time": "2018-03-19T07:33:50.822594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\CAIJIN~1\\AppData\\Local\\Temp\\tmpqsdocf51\\model.ckpt-40000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9415855354659249"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = list(dnn_clf.predict(X_test))\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T07:36:22.657416Z",
     "start_time": "2018-03-19T07:36:20.835630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-19-07:36:21\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\CAIJIN~1\\AppData\\Local\\Temp\\tmpqsdocf51\\model.ckpt-40000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-19-07:36:21\n",
      "INFO:tensorflow:Saving dict for global step 40000: accuracy = 0.94158554, global_step = 40000, loss = 0.29512927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.94158554, 'global_step': 40000, 'loss': 0.29512927}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#或者使用tf自己的评估函数\n",
    "dnn_clf.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用tf清晰地训练DNN\n",
    "如果你想要完全控制网络结构，就要使用TF的低层次API。  \n",
    "## 构造层\n",
    "首先我们需要导入tf库，然后特例化输出和输入数目，每个隐层的神经元数目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T08:44:30.051851Z",
     "start_time": "2018-03-19T08:44:28.330469Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "#然后使用placeholder来表示X，y\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None), name='y')\n",
    "\n",
    "#接着构造网络\n",
    "## 首先构造神经层\n",
    "def neuron_layer(X, n_neurons, name, activation=None):   \n",
    "    with tf.name_scope(name):                               #定义了名字域，将包含整个神经层\n",
    "        n_inputs = int(X.get_shape()[1])                       #查看数组型，并返回第二维的数量，第一维是输入的样例数目\n",
    "        stddev = 2 / np.sqrt(n_inputs)                        #紧接的三行创建了W-权值矩阵，它的型是(n_inputs,n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)  #使用truncated标准高斯分布来随机化权值，使用标准差\n",
    "        W = tf.Variable(init, name='weights')                    #为2/np.sqrt(n_inputs),这个值有助于算法快速收敛\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name='bias')          #初始化每个神经元的偏置项\n",
    "        z = tf.matmul(X, W) + b                             #创建子图计算矩阵乘和加法，\n",
    "        if activation=='relu':                              #激活函数，是否是relu\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "        \n",
    "#下面就是使用构造函数来建立DNN！\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, 'hidden1', activation='relu')\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, 'hidden2', activation='relu')\n",
    "    logits = neuron_layer(hidden2, n_outputs, 'outputs')              #这里并不对输出做softmax，方便管理层结构\n",
    "\n",
    "\n",
    "#然后将下列函数加入构造层末尾  \n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T10:35:16.228154Z",
     "start_time": "2018-03-19T10:35:16.126083Z"
    }
   },
   "outputs": [],
   "source": [
    "#其实TF有用来构造标准神经网络的函数，比如fully_connected()可以建立全连接层，这\n",
    "#个函数更加关注权值和偏置变量，使用合适的初始化，并且默认使用relu，并且提供一\n",
    "#些标准化和正则化的参数,下面是使用他来替换前面的构造网络，两者等价\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2')\n",
    "    logits = fully_connected(hidden2, n_outputs, scope='outputs',\n",
    "                            activation_fn=None)\n",
    "\n",
    "#tf.contrib提供很多有用的函数，不过这个包是实验代码的包，不是主要的API，所以可\n",
    "#能会被移除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了可供运行的网络结构，我们还需要定义一个损失函数才能训练它，我们使用交叉熵来作为损失函数，这样模型的输出的概率数值会尽量低，TF提供计算交叉熵的函数。这个将给对每个样例返回一个1d的tensor，我们接着使用TF的reduce_mean()来计算所有样例的平均交叉熵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T10:35:19.302327Z",
     "start_time": "2018-03-19T10:35:19.291319Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits)                        #默认将标签onehot，输出softmax\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们已经有了网络，有了损失函数，现在我们需要添加梯度下降优化器，来调整模型参数，以最小化损失函数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T10:35:20.835411Z",
     "start_time": "2018-03-19T10:35:20.744349Z"
    }
   },
   "outputs": [],
   "source": [
    "learing_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learing_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一个重要的步骤就是特例化评价模型的函数。我们简单的使用准确度。首先，对于每个样例，判断预测概率最高的项是否符合真正的类，这个可以使用in_top_k()函数，返回一个1Dtensor包含了boolean值，我们要把bool值转换成float然后计算平均值，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T10:35:23.117024Z",
     "start_time": "2018-03-19T10:35:23.104014Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们创建一个节点来初始化所有变量，并且创建一个Saver来把模型保存到硬盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T10:35:25.225535Z",
     "start_time": "2018-03-19T10:35:24.963328Z"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "acc_summary_train = tf.summary.scalar('acc_train', accuracy)\n",
    "acc_summary_test = tf.summary.scalar('acc_test', accuracy)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行层\n",
    "这部分更加短和简单，首先加载MNIST，使用TF自己的MNIST数据，它会自动的抓取，变换，打乱这些数据，还简单的提供一个函数每次只加载mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T10:35:27.535149Z",
     "start_time": "2018-03-19T10:35:27.040797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T11:03:04.983149Z",
     "start_time": "2018-03-19T10:35:28.668952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.9 Test accuracy: 0.9024\n",
      "1 Train accuracy: 0.94 Test accuracy: 0.92030007\n",
      "2 Train accuracy: 0.97999996 Test accuracy: 0.92910004\n",
      "3 Train accuracy: 0.96 Test accuracy: 0.93520004\n",
      "4 Train accuracy: 0.91999996 Test accuracy: 0.94100004\n",
      "5 Train accuracy: 0.96 Test accuracy: 0.94670004\n",
      "6 Train accuracy: 0.96 Test accuracy: 0.94890004\n",
      "7 Train accuracy: 0.91999996 Test accuracy: 0.9538\n",
      "8 Train accuracy: 0.97999996 Test accuracy: 0.9556\n",
      "9 Train accuracy: 0.96 Test accuracy: 0.9584001\n",
      "10 Train accuracy: 0.97999996 Test accuracy: 0.9592\n",
      "11 Train accuracy: 0.97999996 Test accuracy: 0.96300006\n",
      "12 Train accuracy: 0.97999996 Test accuracy: 0.96230006\n",
      "13 Train accuracy: 1.0 Test accuracy: 0.96520007\n",
      "14 Train accuracy: 0.97999996 Test accuracy: 0.9678\n",
      "15 Train accuracy: 1.0 Test accuracy: 0.96830004\n",
      "16 Train accuracy: 1.0 Test accuracy: 0.9696\n",
      "17 Train accuracy: 1.0 Test accuracy: 0.9707\n",
      "18 Train accuracy: 0.96 Test accuracy: 0.97090006\n",
      "19 Train accuracy: 0.97999996 Test accuracy: 0.97270006\n",
      "20 Train accuracy: 0.97999996 Test accuracy: 0.97370005\n",
      "21 Train accuracy: 1.0 Test accuracy: 0.9735001\n",
      "22 Train accuracy: 1.0 Test accuracy: 0.97380006\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.97490007\n",
      "24 Train accuracy: 0.97999996 Test accuracy: 0.97470003\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.9746\n",
      "26 Train accuracy: 1.0 Test accuracy: 0.97540003\n",
      "27 Train accuracy: 0.97999996 Test accuracy: 0.97590005\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.97690004\n",
      "29 Train accuracy: 1.0 Test accuracy: 0.97650003\n",
      "30 Train accuracy: 0.97999996 Test accuracy: 0.97700006\n",
      "31 Train accuracy: 0.97999996 Test accuracy: 0.97650003\n",
      "32 Train accuracy: 1.0 Test accuracy: 0.97740006\n",
      "33 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "34 Train accuracy: 0.97999996 Test accuracy: 0.97800004\n",
      "35 Train accuracy: 0.97999996 Test accuracy: 0.9779\n",
      "36 Train accuracy: 1.0 Test accuracy: 0.9778001\n",
      "37 Train accuracy: 1.0 Test accuracy: 0.97870004\n",
      "38 Train accuracy: 1.0 Test accuracy: 0.9789001\n",
      "39 Train accuracy: 0.97999996 Test accuracy: 0.9778001\n",
      "40 Train accuracy: 1.0 Test accuracy: 0.97830003\n",
      "41 Train accuracy: 1.0 Test accuracy: 0.97850007\n",
      "42 Train accuracy: 1.0 Test accuracy: 0.97870004\n",
      "43 Train accuracy: 1.0 Test accuracy: 0.97940004\n",
      "44 Train accuracy: 0.97999996 Test accuracy: 0.97830003\n",
      "45 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "46 Train accuracy: 0.96 Test accuracy: 0.9789001\n",
      "47 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "48 Train accuracy: 1.0 Test accuracy: 0.97910005\n",
      "49 Train accuracy: 1.0 Test accuracy: 0.97910005\n",
      "50 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "51 Train accuracy: 0.97999996 Test accuracy: 0.97910005\n",
      "52 Train accuracy: 1.0 Test accuracy: 0.97910005\n",
      "53 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "54 Train accuracy: 1.0 Test accuracy: 0.97950006\n",
      "55 Train accuracy: 1.0 Test accuracy: 0.97870004\n",
      "56 Train accuracy: 1.0 Test accuracy: 0.97910005\n",
      "57 Train accuracy: 1.0 Test accuracy: 0.97850007\n",
      "58 Train accuracy: 1.0 Test accuracy: 0.9796001\n",
      "59 Train accuracy: 1.0 Test accuracy: 0.97950006\n",
      "60 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "61 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "62 Train accuracy: 1.0 Test accuracy: 0.9782\n",
      "63 Train accuracy: 1.0 Test accuracy: 0.97910005\n",
      "64 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "65 Train accuracy: 1.0 Test accuracy: 0.9796001\n",
      "66 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "67 Train accuracy: 1.0 Test accuracy: 0.9796001\n",
      "68 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "69 Train accuracy: 1.0 Test accuracy: 0.97910005\n",
      "70 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "71 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "72 Train accuracy: 1.0 Test accuracy: 0.97950006\n",
      "73 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "74 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "75 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "76 Train accuracy: 1.0 Test accuracy: 0.97940004\n",
      "77 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "78 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "79 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "80 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "81 Train accuracy: 1.0 Test accuracy: 0.9796001\n",
      "82 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "83 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "84 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "85 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "86 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "87 Train accuracy: 1.0 Test accuracy: 0.97880006\n",
      "88 Train accuracy: 1.0 Test accuracy: 0.9796001\n",
      "89 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "90 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "91 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "92 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "93 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "94 Train accuracy: 1.0 Test accuracy: 0.98060006\n",
      "95 Train accuracy: 1.0 Test accuracy: 0.9807001\n",
      "96 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "97 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "98 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "99 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "100 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "101 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "102 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "103 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "104 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "105 Train accuracy: 1.0 Test accuracy: 0.9807001\n",
      "106 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "107 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "108 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "109 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "110 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "111 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "112 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "113 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "114 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "115 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "116 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "117 Train accuracy: 1.0 Test accuracy: 0.98060006\n",
      "118 Train accuracy: 1.0 Test accuracy: 0.98050004\n",
      "119 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "120 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "121 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "122 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "123 Train accuracy: 1.0 Test accuracy: 0.98050004\n",
      "124 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "125 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "126 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "127 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "128 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "129 Train accuracy: 1.0 Test accuracy: 0.9807001\n",
      "130 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "131 Train accuracy: 1.0 Test accuracy: 0.9807001\n",
      "132 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "133 Train accuracy: 1.0 Test accuracy: 0.9807001\n",
      "134 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "135 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "136 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "137 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "138 Train accuracy: 1.0 Test accuracy: 0.98060006\n",
      "139 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "140 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "141 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "142 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "143 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "144 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "145 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "146 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "147 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "148 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "149 Train accuracy: 1.0 Test accuracy: 0.98050004\n",
      "150 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "151 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "152 Train accuracy: 1.0 Test accuracy: 0.98060006\n",
      "153 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "154 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "155 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "156 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "157 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "158 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "159 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "160 Train accuracy: 1.0 Test accuracy: 0.98060006\n",
      "161 Train accuracy: 1.0 Test accuracy: 0.98060006\n",
      "162 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "163 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "164 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "165 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "166 Train accuracy: 1.0 Test accuracy: 0.98050004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "168 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "169 Train accuracy: 1.0 Test accuracy: 0.98050004\n",
      "170 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "171 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "172 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "173 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "174 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "175 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "176 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "177 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "178 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "179 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "180 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "181 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "182 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "183 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "184 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "185 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "186 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "187 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "188 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "189 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "190 Train accuracy: 1.0 Test accuracy: 0.98050004\n",
      "191 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "192 Train accuracy: 1.0 Test accuracy: 0.9804\n",
      "193 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "194 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "195 Train accuracy: 1.0 Test accuracy: 0.98020005\n",
      "196 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "197 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "198 Train accuracy: 1.0 Test accuracy: 0.9797\n",
      "199 Train accuracy: 1.0 Test accuracy: 0.97990006\n",
      "200 Train accuracy: 1.0 Test accuracy: 0.97980005\n",
      "201 Train accuracy: 1.0 Test accuracy: 0.98\n",
      "202 Train accuracy: 1.0 Test accuracy: 0.98030007\n",
      "203 Train accuracy: 1.0 Test accuracy: 0.98010004\n",
      "204 Trai"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10042 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#然后定义执行次数和mini-batch的大小\n",
    "n_epochs = 400\n",
    "batch_size = 50\n",
    "\n",
    "#训练模型\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                y: mnist.test.labels})\n",
    "        \n",
    "        \n",
    "        acc_summary_train_ = acc_summary_train.eval(feed_dict={X: X_batch, y: y_batch})  \n",
    "        file_writer.add_summary(acc_summary_train_, epoch)\n",
    "        \n",
    "        acc_summary_test_ = acc_summary_test.eval(feed_dict={X: mnist.test.images,\n",
    "                                           y: mnist.test.labels})   \n",
    "        file_writer.add_summary(acc_summary_test_, epoch)\n",
    "        \n",
    "        print(epoch, 'Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "    \n",
    "    save_path = saver.save(sess, './my_model_final.ckpt')\n",
    "    \n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用已经训练好的神经网络\n",
    "现在已经训练好模型，可以使用它来做预测，需要重用前面的图，然后修改执行层如下，把已经训练好的参数加载下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T11:15:14.270427Z",
     "start_time": "2018-03-19T11:15:12.707918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2')\n",
    "    logits = fully_connected(hidden2, n_outputs, scope='outputs',\n",
    "                            activation_fn=None)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits)                        #默认将标签onehot，输出softmax\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "learing_rate = 0.01\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learing_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "acc_summary_train = tf.summary.scalar('acc_train', accuracy)\n",
    "acc_summary_test = tf.summary.scalar('acc_test', accuracy)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "#修改执行层\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_model_final.ckpt')\n",
    "    X_new_scaled = mnist.test.images\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调优神经网络的参数\n",
    "神经网络的灵活性同样是它的缺点，有太多参数需要调整，不仅是网络的拓扑结构，甚至MLP都可以改变不同层数，不同数量神经元，激活函数类型，权重初始化的逻辑，如何找到最优值呢：  \n",
    "当然可以用gridsearch方式，但是当网络比较大，数据比较多的时候，就没有这么多时间去尝试了。所以最好用randomsearch；另外一个可选项是，使用Oscar这样的工具。  \n",
    "## 隐层数目\n",
    "对于大多数问题，可以从单隐层开始，然后找到一个合理的结果，实际上只含有一个隐层的MLP都能模拟复杂的函数，只要有足够的神经元，长久以来，这对于研究者们很方便，不需要去使用DNN，但是随后他们发现DNN对于参数利用率极高，他们只需要一些神经元就能模拟复杂函数，而且训练也很快。  \n",
    "分层结构不仅仅使DNN快速，还使它能适应新的数据，新的模型甚至可以使用旧模型的参数来初始化其低层。  \n",
    "总的来说，对于多数问题，只需要1到2层隐层。对于一些更复杂的问题，可以逐渐增加，同时也需要大量的训练样例。然而，你几乎不需要从头训练这样的模型，他们有很多相似任务，是已经训练好的网络的重复，这使得训练很快而且不需要那么多数据。\n",
    "## 每个隐层神经元数目\n",
    "显然输入层和输出层的神经元数目对应任务所需要的输入输出数目。对于隐层，一个通常的手段是做成漏斗形，逐层递减。但是这个手段现在不是很通用了，你也可以简单的让所有隐层统一数目，我们甚至还可以在模型过拟合的时候逐层增加神经元。   \n",
    "  \n",
    "一个简单的办法是构造一个层数和神经元数目都大于实际需要的模型，然后通过提前结束来防止过拟合或别的技术。这是“弹力裤”方法：与其浪费时间寻找，不如直接买一条大的，然后收缩到合适大小。\n",
    "# 激活函数\n",
    "大多数情况下隐层可以使用ReLU，或者它的变种。他比其他激活函数更加快速，但是GD算法就不好使用了。幸好的是，多数情况下都不会饱和。  \n",
    "  \n",
    "对于输出层，softmax是不错的选择，对于分类，而对于回归，几乎可以不适用激活函数。  \n",
    "  \n",
    " "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
