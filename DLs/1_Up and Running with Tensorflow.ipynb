{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要\n",
    "tensorflow是一个强有力的数值计算库，特别适应大规模的机器学习任务。  \n",
    "它的规则很简单，首先定义一组python的计算图，然后TensorFlow使用优化的C++代码高效的计算和运行。  \n",
    "更重要的是，可以将图计算分为若干块，并行的在CPUs和GPUs内计算，TensorFlow支持分布计算，可以将超大规模的训练集分为上千的服务器去计算并训练大规模的神经网络，可以使用百万级的样例，百万级的特征，训练百万级参数的神经网络。还有很多优点:   \n",
    "1.可以运行在多种平台，甚至移动平台。  \n",
    "2.提供简单的API叫做TF.contrib.learn，兼容sklearn。  \n",
    "3.提供TF-silm来简化建立、训练和评估神经网络；  \n",
    "4.一些高级的API是基于TF的比如Keras或者Pretty Tensor。  \n",
    "...  \n",
    "# 创建第一个图并运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:26.992249Z",
     "start_time": "2018-03-18T13:50:23.248884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "session是将图放在运算器上运行并且保存数据，可以使用如下方式运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:27.035279Z",
     "start_time": "2018-03-18T13:50:26.997250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的with块中，tf.Session()是默认的session，调用x.initializer.run()等价于tf.get_default_session().run(x.initializer)。  \n",
    "  \n",
    "还有一种运行方式。可以使用global_variable_initialier()。会在计算运行的时候自动载入数值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:27.172375Z",
     "start_time": "2018-03-18T13:50:27.041282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在jupyter中还可以使用InteractiveSession。和一般的session不同，它会自动的创建session并作为默认值，所以都不需要block，但是注意最后要关闭他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:27.309472Z",
     "start_time": "2018-03-18T13:50:27.182382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们可以看到tf编程有两个组成部分，一个是建立计算图，称为构造层construction phase；一个是运行它，称为执行层execution phase。  \n",
    "通常对于ML任务来说，构造层用来表达ML模型和一些需要的计算，执行层用来运行一些循环来重复训练步骤，或者更新参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 管理图\n",
    "任何新建的节点都会自动加入默认的图:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:27.437564Z",
     "start_time": "2018-03-18T13:50:27.315482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数情况下是不错的，但是有时可能需要管理多个单独的图，可以新建图然后暂时的将它作为默认图加入到with中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:27.561651Z",
     "start_time": "2018-03-18T13:50:27.442567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Jupyter中，可能同样的语句会重复执行很多遍，导致默认图里有很多重复节点，一个办法是重启，重新执行，另一个简便的办法是重载默认图tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 节点变量的生命周期\n",
    "当求一个节点的值时，tf会自动的确定一系列与所求节点值需要的节点，并先计算他们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:27.710756Z",
     "start_time": "2018-03-18T13:50:27.565654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子里，代码首先定义了一个简单的图，然后他开始了一个session并且计算y。tf先检测到y是需要x和w的值的，所以先计算他们的值，最后在返回计算y的值。最后计算z的值，又重新再检测到需要x和w的值，所以再次计算两者的值。所以上述过程中x和w的值被计算两次。  \n",
    "在图运行过程中所有节点的除了变量的值都会被忽略，而变量的值保存在session.变量的生命周期就是他初始化直到session关闭。   \n",
    "如果想要高效的计算y和z,可以在一次图计算中计算两者的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:27.903891Z",
     "start_time": "2018-03-18T13:50:27.718762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在单进程的tf中，多个sess之间是无法共享内容的，即使使用同一个图(每个session有自己的变量副本)。在分布式tf中，变量保存在自己的服务器里，而不是在session中，所以可以共享。\n",
    "# 使用tf实现线性回归\n",
    "tf可以处理多输入和任意数量的输出，输入和输出都是多维数组，称为tensors。和numpy的array一样，有type和shape。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:28.335196Z",
     "start_time": "2018-03-18T13:50:27.910898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.72048187e+01],\n",
       "       [ 4.36271667e-01],\n",
       "       [ 9.38740931e-03],\n",
       "       [-1.07066855e-01],\n",
       "       [ 6.44966364e-01],\n",
       "       [-4.11580004e-06],\n",
       "       [-3.77993658e-03],\n",
       "       [-4.23913479e-01],\n",
       "       [-4.37445879e-01]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最主要的好处就是可以在GPU上运行。\n",
    "# 使用梯度下降\n",
    "下面我们将使用梯度下降算法实现，首先先展示手动计算梯度，然后使用tf分autodiff特征让tf自动计算梯度，最后我们使用tf的out-of-box优化器\n",
    "## 手动计算梯度\n",
    "使用random_uniform()函数创建一个能产生随机数的节点，给出数组型和数值范围，类似numpy的rand()；  \n",
    "使用assign()函数创建一个节点，用来分配新的变量，如batch的每步theta    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:29.650126Z",
     "start_time": "2018-03-18T13:50:28.339199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.972075\n",
      "Epoch 100 MSE = 5.0195365\n",
      "Epoch 200 MSE = 4.907174\n",
      "Epoch 300 MSE = 4.876695\n",
      "Epoch 400 MSE = 4.8577304\n",
      "Epoch 500 MSE = 4.843993\n",
      "Epoch 600 MSE = 4.8338566\n",
      "Epoch 700 MSE = 4.826343\n",
      "Epoch 800 MSE = 4.820754\n",
      "Epoch 900 MSE = 4.8165817\n",
      "[[ 0.43165946]\n",
      " [ 0.859406  ]\n",
      " [ 0.15429659]\n",
      " [-0.26576626]\n",
      " [ 0.28124905]\n",
      " [ 0.00804014]\n",
      " [-0.04296798]\n",
      " [-0.612791  ]\n",
      " [-0.5842021 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='error')\n",
    "gradients = 2/m*tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE =', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用autodiff\n",
    "前面的代码运行良好，但是要从MSE上计算得到梯度，在线性回归的情况下，这个是容易求解的，但是在神经网络中很难进行，主要是不方便求解，且容易出错。在没有要求非常效率的情况下。可以使用symbolic differentiation符号微分来自动得到偏导等式。  \n",
    "考虑$f(x)=e^{e^{e^x}}$,他的导数是$f'(x)=e^xe^{e^x}e^{e^{e^x}}$,如果代码中将他们分别都计算出来，显然是不够效率的。  \n",
    "一个效率的办法是计算$e^x$,再计算$e^{e^x}$,最后计算$e^{e^{e^x}}$,这样既得到了$f(x)$，也通过三个乘积得到$f'(x)$。  \n",
    "根据这个基本想法，可以将9次指数计算减少为3次。  \n",
    "但是函数为任意情况下，就不容易寻找这样的偏导数，比如这样:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:29.670140Z",
     "start_time": "2018-03-18T13:50:29.656130Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_fun(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好消息是，tf的autodiff特征可以解决这个问题，自动高效地求出梯度，简单的替换掉前面的梯度函数，仅仅使用下面的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:30.819953Z",
     "start_time": "2018-03-18T13:50:29.676145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 7.3364763\n",
      "Epoch 100 MSE = 5.2583647\n",
      "Epoch 200 MSE = 5.0591235\n",
      "Epoch 300 MSE = 4.9846625\n",
      "Epoch 400 MSE = 4.935235\n",
      "Epoch 500 MSE = 4.8996534\n",
      "Epoch 600 MSE = 4.873806\n",
      "Epoch 700 MSE = 4.854994\n",
      "Epoch 800 MSE = 4.841282\n",
      "Epoch 900 MSE = 4.831274\n",
      "[[-0.366812  ]\n",
      " [ 0.82088834]\n",
      " [ 0.16848813]\n",
      " [-0.15294155]\n",
      " [ 0.17033917]\n",
      " [ 0.01370264]\n",
      " [-0.04334717]\n",
      " [-0.5455075 ]\n",
      " [-0.510498  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='error')\n",
    "\n",
    "#gradients = 2/m*tf.matmul(tf.transpose(X), error)\n",
    "\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE =', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里gradients()函数操作一个op(这里是mse)和一个变量列表(这里是theta)，并且建立一个列表的ops来计算op对每个列表变量的偏导数，所以gradients节点将计算一个偏导向量,tf使用reverse-mode autodiff，效率和准备度在多输入少输出情况下很理想，常用于神经网络，它计算偏导向量需要遍历$n_{output}+1$次图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用优化器\n",
    "所以tf可以直接计算梯度，但是这还早，tf还提供了一系列的优化器，包括了梯度下降优化器。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:31.586494Z",
     "start_time": "2018-03-18T13:50:30.824956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 8.908798\n",
      "Epoch 100 MSE = 4.9779143\n",
      "Epoch 200 MSE = 4.8793097\n",
      "Epoch 300 MSE = 4.8600364\n",
      "Epoch 400 MSE = 4.8470325\n",
      "Epoch 500 MSE = 4.837239\n",
      "Epoch 600 MSE = 4.829777\n",
      "Epoch 700 MSE = 4.82406\n",
      "Epoch 800 MSE = 4.8196564\n",
      "Epoch 900 MSE = 4.8162475\n",
      "[[ 0.6451824 ]\n",
      " [ 0.8928189 ]\n",
      " [ 0.15245952]\n",
      " [-0.34404847]\n",
      " [ 0.3526465 ]\n",
      " [ 0.00693861]\n",
      " [-0.04352513]\n",
      " [-0.5948129 ]\n",
      " [-0.57084596]]\n"
     ]
    }
   ],
   "source": [
    "#gradients = 2/m*tf.matmul(tf.transpose(X), error)\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "#training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, 'MSE =', mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要使用别的优化器只需要更改一行就行了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 给算法提供数据\n",
    "我们尝试把前面的算法修改为Mini-batch形式。我们需要把X,y替换为mini-batch的迭代器形式，最简方法就是修改节点，使用placeholder节点，这类节点的特点是不实际执行任何计算，它们只在实际运行的时候输出用户想要输出的值，典型应用在tf训练模型的时候提供训练数据。如果不将节点变为placeholder节点，运行时将会弹出exception。  \n",
    "  \n",
    "建立placeholder节点，需要调用placeholder()，并且设置输出tensor的数据类型，也可以设置数组形状，这项的默认值是any size。比如，下面的代码建立了placeholder节点A，以及一个节点B=A+5.当我们求B得值时，我们将对eval()使用feed_dict来特例化A的值，注意A有2级，即2维,但是可以是任意级，以及3列，必须是3列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:31.663548Z",
     "start_time": "2018-03-18T13:50:31.590497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实施mini-batch，应该如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:31.971767Z",
     "start_time": "2018-03-18T13:50:31.668553Z"
    }
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')  \n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')  \n",
    "   \n",
    "batch_size = 100 \n",
    "n_batch = int(np.ceil(m / batch_size))  \n",
    "  \n",
    "def fetch_batch(epoch, batch_index, batch_size):  \n",
    "    X_batch = scaled_housing_data_plus_bias[epoch * batch_index : epoch * batch_index + batch_size]\n",
    "    y_batch = housing.target.reshape(-1, 1)[epoch * batch_index : epoch * batch_index + batch_size]\n",
    "    return X_batch, y_batch  \n",
    "  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    \n",
    "    for batch_index in range(batch_size):  \n",
    "        X_batch, y_batch = fetch_batch(epoch, batch_index , batch_size)  \n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  \n",
    "    best_theta = theta.eval()  \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存及重载模型\n",
    "一旦训练好模型，便可以将他们保存在硬盘中，用来应用到其他程序或是比较模型。或者是保存进度以免计算机故障而前功尽弃。  \n",
    "tf保存和重载模型十分简单，只需要在构造层的末尾创建一个 saver节点，然后执行层调用save()函数：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:36.943680Z",
     "start_time": "2018-03-18T13:50:31.978771Z"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        \n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重载模型只需要在构造层的最后加入saver，然后在执行层的最前面用restore()替换init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:37.097762Z",
     "start_time": "2018-03-18T13:50:36.960663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/tmp/my_model_final.ckpt')\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下saver使用自己的名字保存和重载所有变量，也可以自己更改：  \n",
    "saver = tf.train.Saver({'weights': theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard图的可视化和训练曲线\n",
    "第一步是微调程序，写入图的定义和一些状态量到日志中，这样tfBoard可以读取它们。需要使用不同的日志名当编程的时候。最佳办法就是加入时间戳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:37.496073Z",
     "start_time": "2018-03-18T13:50:37.103764Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "root_logdir = 'tf_Logs'\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "#然后将下列函数加入构造层末尾  \n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一行在图中创建了一个求MSE并将其写入名为summary的TsBoard-兼容的二进制日志的节点。第二行创建了一个FileWriter，用来将summary写入到指定日志文件中，第一个参数是日志路径，第二个是用来可视化的图。这里的结果就是创建一个原来不存在的日志路径并且将图写入到一个名为events file事件文件的二进制文件中。  \n",
    "下面就是更新执行层，并且对mse_summary节点在训练时求值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:37.812297Z",
     "start_time": "2018-03-18T13:50:37.499077Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for batch_index in range(n_batch):\n",
    "        X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "        if batch_index % 10 == 0: #避免每步都保存降低训练效率\n",
    "            summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            step = epoch * n_batch +batch_index\n",
    "            file_writer.add_summary(summary_str, step)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "#最后关闭FileWriter\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码创建了一个日志路径，并且把事件写入，包括图定义和MSE的值。  \n",
    "现在可以打开TSboard服务器了。需要在环境中激活它，然后使用tensorboard命令打开它，下面是开启sever，监听在6006端口  \n",
    "\\$ source env/bin/activate  \n",
    "\\$ tensorboard --logdir tf_logs/  \n",
    "Starting TensorBoard on port 6006  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 名字域\n",
    "\n",
    "在处理复杂模型，比如神经网络时，图可能会聚合成一堆，其中有上万个节点，为了避免这种情况，可以使用name scopes来把相关的图放在一起，比如我们把前面的error 和mse放在一个叫做loss的名字域中，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:37.835314Z",
     "start_time": "2018-03-18T13:50:37.818301Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss') as scope:\n",
    "    error = y_pred - y \n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模块化\n",
    "假如想要建立一个图，其中输出是两个修正线性单元rectified lineear units(ReLU).一个ReLU计算输入的线性函数，如果结果是正就输出正数，否则输出0：  \n",
    "$h_{\\mathbf w,b}(\\mathbf X) = max(\\mathbf X\\cdot\\mathbf w+b,0)$  \n",
    "下面是这个单元的代码：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:38.023446Z",
     "start_time": "2018-03-18T13:50:37.846322Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name='weights1')\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name='weights2')\n",
    "b1 = tf.Variable(0.0, name='bias1')\n",
    "b2 = tf.Variable(0.0, name='bias2')\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name='z1')\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name='z2')\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name='relu1')\n",
    "relu2 = tf.maximum(z2, 0., name='relu2')\n",
    "\n",
    "output = tf.add(relu1, relu2, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样的代码含有大量重复。不易维护而且易出错。特别是想增加更多这类单元的时候，tf可以让你原理DRY(don't repeat yourself自我重复)：简单的构造一个模块:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:38.273624Z",
     "start_time": "2018-03-18T13:50:38.028449Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "    return tf.maximum(z, 0., name='relu')\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=((None, n_features)), name='X')\n",
    "relu = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relu, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf自动检测重名，并且加入后缀,这样tf定义一系列的变量来减少簇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:39.153365Z",
     "start_time": "2018-03-18T13:50:38.278627Z"
    }
   },
   "outputs": [],
   "source": [
    "#然后将下列函数加入构造层末尾  \n",
    "sess = tf.InteractiveSession()\n",
    "file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "init.run()\n",
    "\n",
    "sess.close()\n",
    "#最后关闭FileWriter\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共享变量\n",
    "如果想要在一个图之间的若干部位共享变量，一个简单的方法是创建他，然后将它作为函数的参数，比如你想要控制ReLU的阈值，在所有的ReLU间共享阈值，可以如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:39.573650Z",
     "start_time": "2018-03-18T13:50:39.157350Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X, threshold):\n",
    "    with tf.name_scope('relu'):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "        b = tf.Variable(0.0, name='bias')\n",
    "        z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "        return tf.maximum(z, threshold, name='max')\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "threshold = tf.Variable(0.0, name='threshold')\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = [relu(X,threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name='output')\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "sess.close()\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个结果看着不错，现在可以对所有ReLU的阈值进行调整，然而，如果有很多歌这样的共享变量，操作起来也很麻烦。tf提供一个选项，可以比较清晰地解决。思路是使用get_variable()函数来创建一个共享变量，不管他是否真实存在，想要的操作行为由当前的vaiable_scope()的特性操控。比如说下面的代码创建了一个relu/threshold变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:50:39.611675Z",
     "start_time": "2018-03-18T13:50:39.587655Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('relu'):\n",
    "    threshold = tf.get_variable('threshold', shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里如果变量在get_variable前已经创建，代码将会弹出异常。这种方式防止错误的重用变量，如果想要重用，需要在scope中设置reuse属性为真(这种情况下不需要特例化数组型和初始化):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T13:55:55.044687Z",
     "start_time": "2018-03-18T13:55:55.037683Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('relu', reuse=True):\n",
    "    threshold = tf.get_variable('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个代码将会取到实际存在的relu/threshold值或者在不存在值时或者没有使用get.variable()来创建时将弹出异常。  \n",
    "还有一种办法是，可以设置reuse在with代码块中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T14:00:28.601068Z",
     "start_time": "2018-03-18T14:00:28.595064Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('relu') as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable('threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你就可以如下共享变量了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T14:08:54.305199Z",
     "start_time": "2018-03-18T14:08:53.905916Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.variable_scope('relu', reuse=True):\n",
    "        threshold = tf.get_variable('threshold')\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "        b = tf.Variable(0.0, name='bias')\n",
    "        z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "        return tf.maximum(z, threshold, name='max')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "with tf.variable_scope('relu'):  #create the variable\n",
    "    threshold = tf.get_variable('threshold', shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name='output')\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "sess.close()\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里首先定义了relu函数，创建了relu/threshold变量。并且创建了5个relu，其中重用了变量。  \n",
    "可惜的是这里阈值变量要在relu函数外创建。为了解决这点，下面的代码在relu函数第一次调用的时候就创建阈值变量，并在后面复用他。这里第一次使用时reuse是否的，而后面几次修改为真。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T14:22:54.415258Z",
     "start_time": "2018-03-18T14:22:54.049983Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    threshold = tf.get_variable('threshold',shape=(),\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X, w), b, name='z')\n",
    "    return tf.maximum(z, threshold, name='max')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope('relu', reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name='output')\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "sess.close()\n",
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
